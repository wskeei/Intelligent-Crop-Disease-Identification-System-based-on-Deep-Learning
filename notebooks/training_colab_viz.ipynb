{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŒ¾ CropVision-AI Advanced Training (Colab Edition)\n",
                "\n",
                "This notebook runs the advanced training pipeline for the Crop Disease Identification System. \n",
                "It connects to your GitHub repository, downloads the dataset, and runs the training with live visualization of Loss and Accuracy.\n",
                "\n",
                "**Features:**\n",
                "- MobileNetV3 (Student) + ResNet50 (Teacher) Distillation\n",
                "- Live Training Plots (Loss & Accuracy)\n",
                "- Structured Pruning & Quantization Aware Training (QAT)\n",
                "- GPU Acceleration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Setup Environment\n",
                "\n",
                "# Clone the repository (if not already cloned)\n",
                "import os\n",
                "if not os.path.exists('Intelligent-Crop-Disease-Identification-System-based-on-Deep-Learning'):\n",
                "    !git clone https://github.com/wskeei/Intelligent-Crop-Disease-Identification-System-based-on-Deep-Learning.git\n",
                "    print(\"Repository cloned.\")\n",
                "else:\n",
                "    print(\"Repository already exists.\")\n",
                "\n",
                "# Install dependencies\n",
                "!pip install torch torchvision matplotlib tqdm\n",
                "\n",
                "# Check GPU\n",
                "import torch\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Device: {device}\")\n",
                "if device.type == 'cpu':\n",
                "    print(\"âš ï¸ Warning: Running on CPU. Go to Runtime > Change runtime type > T4 GPU for faster training.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Download Dataset\n",
                "\n",
                "# We use the setup logic from the repo, adapted for python execution\n",
                "import os\n",
                "\n",
                "DATA_DIR = \"Intelligent-Crop-Disease-Identification-System-based-on-Deep-Learning/backend/training/data\"\n",
                "os.makedirs(DATA_DIR, exist_ok=True)\n",
                "\n",
                "if not os.path.exists(os.path.join(DATA_DIR, 'PlantVillage')) and len(os.listdir(DATA_DIR)) < 5:\n",
                "    print(\"Downloading PlantVillage Dataset...\")\n",
                "    !wget -q https://github.com/spMohanty/PlantVillage-Dataset/archive/refs/heads/master.zip -O plantvillage.zip\n",
                "    !unzip -q plantvillage.zip\n",
                "    \n",
                "    # Move files to expected directory structure\n",
                "    # The zip contains PlantVillage-Dataset-master/raw/color/[classes]\n",
                "    # We want them in DATA_DIR/[classes]\n",
                "    print(\"Organizing dataset...\")\n",
                "    !cp -r PlantVillage-Dataset-master/raw/color/* \"{DATA_DIR}\"\n",
                "    \n",
                "    # Cleanup\n",
                "    !rm -rf PlantVillage-Dataset-master plantvillage.zip\n",
                "    print(\"Dataset ready.\")\n",
                "else:\n",
                "    print(\"Dataset appears to be present.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Import Modules\n",
                "\n",
                "import sys\n",
                "import time\n",
                "import copy\n",
                "import matplotlib.pyplot as plt\n",
                "from IPython.display import clear_output\n",
                "from tqdm.notebook import tqdm\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import DataLoader\n",
                "from torchvision import datasets, transforms\n",
                "\n",
                "# Add the training directory to sys.path so we can import local modules\n",
                "TRAINING_PATH = os.path.abspath(\"Intelligent-Crop-Disease-Identification-System-based-on-Deep-Learning/backend/training\")\n",
                "if TRAINING_PATH not in sys.path:\n",
                "    sys.path.append(TRAINING_PATH)\n",
                "\n",
                "# Now we can import from the repo's code\n",
                "from models import get_student_model, get_teacher_model\n",
                "from distillation import KnowledgeDistillationLoss\n",
                "from pruning import apply_structured_pruning, remove_pruning_reparameterization\n",
                "from quantization import prepare_model_for_qat, convert_qat_model\n",
                "\n",
                "print(\"Modules imported successfully.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Define Visualization Helper\n",
                "\n",
                "class TrainingVisualizer:\n",
                "    def __init__(self):\n",
                "        self.train_losses = []\n",
                "        self.val_accs = []\n",
                "        self.epochs = []\n",
                "        \n",
                "    def update(self, epoch, train_loss, val_acc):\n",
                "        self.epochs.append(epoch)\n",
                "        self.train_losses.append(train_loss)\n",
                "        self.val_accs.append(val_acc)\n",
                "        \n",
                "        clear_output(wait=True)\n",
                "        fig, ax1 = plt.subplots(figsize=(10, 5))\n",
                "        \n",
                "        color = 'tab:red'\n",
                "        ax1.set_xlabel('Epoch')\n",
                "        ax1.set_ylabel('Loss', color=color)\n",
                "        ax1.plot(self.epochs, self.train_losses, color=color, label='Train Loss', marker='o')\n",
                "        ax1.tick_params(axis='y', labelcolor=color)\n",
                "        ax1.grid(True, alpha=0.3)\n",
                "        \n",
                "        ax2 = ax1.twinx()  \n",
                "        color = 'tab:blue'\n",
                "        ax2.set_ylabel('Accuracy (%)', color=color)\n",
                "        ax2.plot(self.epochs, self.val_accs, color=color, label='Val Acc', marker='s')\n",
                "        ax2.tick_params(axis='y', labelcolor=color)\n",
                "        \n",
                "        plt.title('Training Progress')\n",
                "        plt.tight_layout()\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Configuration & Training Loop\n",
                "\n",
                "# Config\n",
                "BATCH_SIZE = 32\n",
                "LEARNING_RATE = 0.001\n",
                "EPOCHS_DISTILLATION = 15\n",
                "PRUNE_AMOUNT = 0.2\n",
                "DEBUG_MODE = False # Set to True for a quick 1-epoch test\n",
                "\n",
                "if DEBUG_MODE:\n",
                "    EPOCHS_DISTILLATION = 1\n",
                "    \n",
                "# Data Transforms\n",
                "transform = transforms.Compose([\n",
                "    transforms.Resize(256),\n",
                "    transforms.CenterCrop(224),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "print(\"Loading Data...\")\n",
                "full_dataset = datasets.ImageFolder(DATA_DIR, transform=transform)\n",
                "num_classes = len(full_dataset.classes)\n",
                "print(f\"Classes: {num_classes}\")\n",
                "\n",
                "train_size = int(0.8 * len(full_dataset))\n",
                "val_size = len(full_dataset) - train_size\n",
                "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
                "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
                "\n",
                "# Initialize Models\n",
                "student = get_student_model(num_classes).to(device)\n",
                "teacher = get_teacher_model(num_classes).to(device) # We use ImageNet weights for teacher here\n",
                "\n",
                "criterion = KnowledgeDistillationLoss(alpha=0.5, temperature=3.0)\n",
                "optimizer = optim.Adam(student.parameters(), lr=LEARNING_RATE)\n",
                "viz = TrainingVisualizer()\n",
                "\n",
                "def train_one_epoch(model, teacher, loader):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
                "    for images, labels in pbar:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        student_logits = model(images)\n",
                "        with torch.no_grad():\n",
                "            teacher_logits = teacher(images)\n",
                "            \n",
                "        loss = criterion(student_logits, teacher_logits, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += loss.item()\n",
                "        _, predicted = student_logits.max(1)\n",
                "        total += labels.size(0)\n",
                "        correct += predicted.eq(labels).sum().item()\n",
                "        \n",
                "        pbar.set_postfix({'loss': loss.item()})\n",
                "        \n",
                "    return running_loss / len(loader), 100. * correct / total\n",
                "\n",
                "def validate(model, loader):\n",
                "    model.eval()\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    with torch.no_grad():\n",
                "        for images, labels in loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            outputs = model(images)\n",
                "            _, predicted = outputs.max(1)\n",
                "            total += labels.size(0)\n",
                "            correct += predicted.eq(labels).sum().item()\n",
                "    return 100. * correct / total\n",
                "\n",
                "# --- Phase 1: Distillation Training ---\n",
                "print(\"Starting Distillation Training...\")\n",
                "best_acc = 0.0\n",
                "for epoch in range(EPOCHS_DISTILLATION):\n",
                "    loss, train_acc = train_one_epoch(student, teacher, train_loader)\n",
                "    val_acc = validate(student, val_loader)\n",
                "    \n",
                "    viz.update(epoch + 1, loss, val_acc)\n",
                "    \n",
                "    if val_acc > best_acc:\n",
                "        best_acc = val_acc\n",
                "        torch.save(student.state_dict(), \"student_best.pth\")\n",
                "        \n",
                "print(f\"Distillation Complete. Best Accuracy: {best_acc:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Pruning & Finetuning\n",
                "\n",
                "print(\"Applying Structured Pruning...\")\n",
                "student.load_state_dict(torch.load(\"student_best.pth\"))\n",
                "apply_structured_pruning(student, amount=PRUNE_AMOUNT)\n",
                "\n",
                "# Finetune\n",
                "print(\"Fine-tuning Pruned Model...\")\n",
                "optimizer = optim.Adam(student.parameters(), lr=LEARNING_RATE * 0.1)\n",
                "for epoch in range(3):\n",
                "    loss, _ = train_one_epoch(student, teacher, train_loader)\n",
                "    val_acc = validate(student, val_loader)\n",
                "    print(f\"Pruning FT Epoch {epoch+1} - Val Acc: {val_acc:.2f}%\")\n",
                "\n",
                "remove_pruning_reparameterization(student)\n",
                "torch.save(student.state_dict(), \"student_pruned.pth\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Quantization Aware Training (QAT)\n",
                "\n",
                "print(\"Starting QAT...\")\n",
                "# QAT typically requires CPU for some backend configs in PyTorch, or careful specific CUDA setup\n",
                "# We will stick to the script's strategy: prepare on CPU/appropriate device\n",
                "\n",
                "student.to('cpu')\n",
                "student.train()\n",
                "prepare_model_for_qat(student)\n",
                "\n",
                "optimizer = optim.Adam(student.parameters(), lr=LEARNING_RATE * 0.01)\n",
                "criterion_ce = torch.nn.CrossEntropyLoss()\n",
                "\n",
                "# Simple QAT loop\n",
                "for epoch in range(3):\n",
                "    student.train()\n",
                "    running_loss = 0.0\n",
                "    pbar = tqdm(train_loader, desc=f\"QAT Epoch {epoch+1}\", leave=True)\n",
                "    for images, labels in pbar:\n",
                "        optimizer.zero_grad()\n",
                "        outputs = student(images)\n",
                "        loss = criterion_ce(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        running_loss += loss.item()\n",
                "        pbar.set_postfix({'loss': loss.item()})\n",
                "        \n",
                "quantized_model = convert_qat_model(student)\n",
                "print(\"QAT Complete.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Save & Download\n",
                "torch.save(quantized_model.state_dict(), \"quantized_mobilenet.pth\")\n",
                "\n",
                "from google.colab import files\n",
                "files.download('quantized_mobilenet.pth')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}